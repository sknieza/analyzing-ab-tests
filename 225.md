Congratulations on getting to the end of Sprint 2! You have almost completed a Sprint on a rather nuanced topic - statistical inference. You now know how to estimate population parameters from samples, how to express uncertainty using confidence intervals and how to test hypotheses. Now it is time to apply these skills on some real-world datasets.

### Objectives
- Practice analyzing experiments.
- Practice conducting statistical tests and calculating confidence intervals.
- Practice communicating results of an A/B test.
- Practice creating dashboards.

### Requirements

Your task is to analyze two datasets - [Fast Food Marketing Campaign A\B Test dataset](https://www.kaggle.com/datasets/chebotinaa/fast-food-marketing-campaign-ab-test) and [Cookie Cats A/B Test](https://www.kaggle.com/datasets/mursideyarkin/mobile-games-ab-testing-cookie-cats) dataset. To do that, you will have to create two analysis Jupyter notebooks, both with the following structure:

1. Goal. Based on the dataset description in Kaggle, describe the goal of the A/B test. Give any necessary context.
2. Target metric. Choose a target metric and describe how it measures progress towards the stated goal of the A/B test.
3. Calculations.
    - In your notebook, aggregate and prepare data as needed. Report key information about the A/B test, including the sample size and variant proportions.
    - Conduct statistical tests. Describe which test you have chosen for each metric and why.
    - Report the estimated treatment effect, confidence interval and p-value. For one of the datasets, compute the confidence interval both analytically and using bootstrap and report both intervals.
4. Decision. Describe clearly what your calculations imply and which decision you recommend. Use visualizations to show differences in the metric between groups and confidence intervals.

In addition to the notebooks, create a dashboard with at least two plots. One of the plots should illustrate how the target metric changes across the weeks in the Fast Food Marketing Campaign A/B test.

Some tips:
1. There are several statistical tests that you might need during this project - Student’s t-test, binomial test and the chi-square test. Here are some good resources on them:
    - [Student’s t-test](https://www.youtube.com/watch?v=pTmLQvMM-1M) and [Scipy documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html)
    - [Chi-square test](https://www.youtube.com/watch?v=jABsbNBPXIk) and [Scipy documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html)
2. For statistical tests, feel free to check your numbers against those given by the [Evan Miller A/B Test Calculator.](https://www.evanmiller.org/ab-testing/)
3. You will notice that the number of samples in each variant is not the same. Some discrepancy should be expected due to randomness, but large differences, when they are not expected, are a symptom of a bug in the way units were randomized. This is known as [sample ratio mismatch (SRM)](https://en.wikipedia.org/wiki/Sample_ratio_mismatch). Before proceeding with the analysis of the test, test for sample ratio mismatch using the chi-square test.

### Evaluation Criteria

- Adherence to the requirements. How well did you meet the requirements?
- Visualization quality, in both the notebooks and the dashboard. Did you use charts effectively to support your recommendations? Are your visualizations properly labeled? Did you use colors effectively? Did you adhere to the principle of proportional ink?
- Interpretation of results. Are you able to explain where the uncertainty in your estimates comes from? Are you able to measure and communicate that uncertainty in the form of a confidence interval?
- Text clarity. Are you able to clearly communicate the goals of the A/B test, why you chose specific metrics and the results of the A/B test?
- Code quality. Was your code well-structured? Did you use the appropriate levels of abstraction? Did you remove commented out and unused code? Did you adhere to PEP8?
- Code performance. Did you use the suitable algorithms and data structures to solve the problems?

### Project Review

During your project review, you should present your project as if talking to a manager and senior data scientist working in your team. You will have to find the right balance between explaining your insights intuitively and diving deeper into the technical aspects of your work. 

During a project review, you may get asked questions that test your understanding of covered topics. Here are some examples:
- What is a confidence interval? Why do we need it? Why is it not sufficient to just report the point estimates?
- Why is metric choice so important in A/B tests?
- What is a treatment effect?
- What are randomized controlled experiments? Why do we need them?

For general project review guidelines, please refer to [this document.](https://turingcollege.atlassian.net/wiki/spaces/DLG/pages/537395951/Peer+expert+reviews+corrections)

### Additional Resources for this Sprint

- [ ] [Andrew Gelman Blog](https://statmodeling.stat.columbia.edu/)
- [ ] [Ronny Kohavi, Diane Tang, Ya Xu - Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing](https://www.amazon.com/Trustworthy-Online-Controlled-Experiments-Practical/dp/1108724264/ref=sr_1_1?crid=373750Q3U8VVV&dib=eyJ2IjoiMSJ9.3n8P8d0FvU0dKN_lC7cMIJu1deoCoHMU0vW71w4IzsC6u9g2MZvSmiYjIP-M85e3R2592_GeBL0OZVT6Aychkuqy27jXDezsatSW6c1GbW2X3R7rsHB4U1-5T0o0RX0C1V0mVJvFn6C47q5DoqX7KCtJpvqCbxO64_L4FMkGzOb0lHopN5f6pFzfRo2RkPSi.nVnzJkHQ7NMXoh30e3U1UBeSQeRYqEgO6J088qb5Bb4&dib_tag=se&keywords=trustworthy+online+controlled+experiments&qid=1713720755&sprefix=trustworty+online+%2Caps%2C173&sr=8-1)
- [ ] [Harvard’s Statistics 110: Probability course](https://projects.iq.harvard.edu/stat110)
- [ ] [Richard McElreath - Statistical Rethinking](https://xcelab.net/rm/)
- [ ] [Allen Downey Blog](https://www.allendowney.com/blog/)
